Q-learning做接水果游戏，游戏有游戏动画在gifs/catch.gif里面
代码主要参考[这里](https://github.com/farizrahman4u/qlearning4k)，改变之处主要是增加了注释，并稍微修改了风格。

游戏内容是，一个水果垂直落下来，下面有个盘子，每次可以选择左移、右移、不动，来接住水果。

考虑一个游戏 **G**. 由一个函数表示。该有两个输入，当前状态**G**，一个动作**a**。函数输出一个新状态**S'** 和一个奖励**r**

那么动作**a**是怎么得到的呢？显然，对于游戏而言，动作**a**只能与当前状态**G**有关。即，我们需要训练一个函数**F**，满足
**F(S) = a**即输入是当前状态，输出是动作**a**

对于接水果游戏，有三个动作，左移、右移、不动。故我们设计一个神经网络**M**，输入是当前状态**S**，输出神经元有三个

**M(S) = {q1, q2, q3}**

这里 **q1, q2, q3** 就是网络**M**的三个输出得分。我们选择分数最大的那个，作为基于当前状态**S**的预测动作

**F(S) = argmax(M(S))**

下面定义Q函数：
Q函数的输入是当前状态**S**，和依据当前状态获得的动作**a**，输出是该动作产生的反馈r，或者叫得分r。如果该动作让游戏赢了，则令r=1，失败则r=-1，暂时无法判断输赢则r=0

显然
**Q(S, a) = r + Q(S', a')**

其中**S’**是新状态，**a’**是基于新状态的新动作

这是一个递归函数

我们可以引入一个小于1.0 的gamma，来把当前分数r置一个更大的权重，

**Q(S, a) = r + gamma * Q(S', a')**

